# Core dependencies for Llama.cpp Server
fastapi>=0.104.0
uvicorn[standard]>=0.24.0
pydantic>=2.5.0
python-multipart>=0.0.6
aiohttp>=3.9.0
psutil>=5.9.0
jwt
bcrypt==5.0.0
# Llama.cpp integration
# For CPU-only installation: pip install llama-cpp-python
# For GPU support (CUDA): pip install llama-cpp-python --prefer-binary --extra-index-url=https://jllllll.github.io/llama-cpp-python-cuBLAS-wheels/AVX2/cu118
# For GPU support (Metal): pip install llama-cpp-python --prefer-binary --extra-index-url=https://jllllll.github.io/llama-cpp-python-metal-wheels/AVX2
llama-cpp-python>=0.2.0

# Optional performance dependencies
numpy>=1.24.0
scipy>=1.10.0

# Optional monitoring dependencies
prometheus-client>=0.19.0
structlog>=23.1.0