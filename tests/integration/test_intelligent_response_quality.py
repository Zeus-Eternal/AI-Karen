"""
Integration Tests for Intelligent Response Quality and Accuracy

Tests the quality, accuracy, and consistency of intelligent error responses
generated by the premium response system.
"""

import pytest
import asyncio
from datetime import datetime, timezone, timedelta
from typing import Dict, Any, List
from unittest.mock import patch, AsyncMock, MagicMock

from ai_karen_engine.services.error_response_service import (
    ErrorResponseService,
    ErrorCategory,
    ErrorSeverity,
    IntelligentErrorResponse
)
from ai_karen_engine.services.ai_orchestrator import AIOrchestrator
from ai_karen_engine.auth.provider_health_monitor import ProviderHealthMonitor


@pytest.fixture
def error_service():
    """Create error response service instance."""
    return ErrorResponseService()


@pytest.fixture
def ai_orchestrator():
    """Mock AI orchestrator."""
    with patch('ai_karen_engine.services.error_response_service.AIOrchestrator') as mock:
        instance = AsyncMock()
        mock.return_value = instance
        yield instance


@pytest.fixture
def health_monitor():
    """Mock provider health monitor."""
    with patch('ai_karen_engine.services.error_response_service.ProviderHealthMonitor') as mock:
        instance = MagicMock()
        mock.return_value = instance
        yield instance


class TestErrorClassificationAccuracy:
    """Test accuracy of error classification and categorization."""

    def test_openai_api_key_classification_accuracy(self, error_service):
        """Test accurate classification of OpenAI API key errors."""
        test_cases = [
            "OPENAI_API_KEY not set in environment",
            "OpenAI API key is missing",
            "api key not found for openai",
            "Missing OpenAI API key in configuration",
            "OPENAI_API_KEY environment variable not set"
        ]
        
        for error_message in test_cases:
            response = error_service.analyze_error(
                error_message=error_message,
                provider_name="OpenAI"
            )
            
            assert response.category == ErrorCategory.API_KEY_MISSING
            assert response.severity == ErrorSeverity.HIGH
            assert "OpenAI" in response.title
            assert "OPENAI_API_KEY" in " ".join(response.next_steps)
            assert response.contact_admin is False
            assert response.help_url is not None

    def test_anthropic_api_key_classification_accuracy(self, error_service):
        """Test accurate classification of Anthropic API key errors."""
        test_cases = [
            "ANTHROPIC_API_KEY not found",
            "Anthropic API key missing",
            "anthropic api key not configured",
            "Missing ANTHROPIC_API_KEY environment variable"
        ]
        
        for error_message in test_cases:
            response = error_service.analyze_error(
                error_message=error_message,
                provider_name="Anthropic"
            )
            
            assert response.category == ErrorCategory.API_KEY_MISSING
            assert response.severity == ErrorSeverity.HIGH
            assert "Anthropic" in response.title
            assert "ANTHROPIC_API_KEY" in " ".join(response.next_steps)

    def test_session_expiry_classification_accuracy(self, error_service):
        """Test accurate classification of session expiry errors."""
        test_cases = [
            ("Token has expired", 401),
            ("JWT token expired", 401),
            ("Session expired", 401),
            ("Access token is no longer valid", 401),
            ("Authentication token expired", 401)
        ]
        
        for error_message, status_code in test_cases:
            response = error_service.analyze_error(
                error_message=error_message,
                status_code=status_code
            )
            
            assert response.category == ErrorCategory.AUTHENTICATION
            assert response.severity == ErrorSeverity.MEDIUM
            assert "Session" in response.title or "Token" in response.title
            assert any("log in" in step.lower() for step in response.next_steps)
            assert response.contact_admin is False

    def test_rate_limit_classification_accuracy(self, error_service):
        """Test accurate classification of rate limit errors."""
        test_cases = [
            ("Rate limit exceeded", 429),
            ("Too many requests", 429),
            ("API rate limit hit", 429),
            ("Request quota exceeded", 429),
            ("Rate limiting in effect", 429)
        ]
        
        for error_message, status_code in test_cases:
            response = error_service.analyze_error(
                error_message=error_message,
                status_code=status_code,
                provider_name="OpenAI"
            )
            
            assert response.category == ErrorCategory.RATE_LIMIT
            assert response.severity == ErrorSeverity.MEDIUM
            assert "Rate" in response.title or "Limit" in response.title
            assert response.retry_after is not None
            assert response.retry_after > 0
            assert any("wait" in step.lower() for step in response.next_steps)

    def test_database_error_classification_accuracy(self, error_service):
        """Test accurate classification of database errors."""
        test_cases = [
            'relation "users" does not exist',
            "Database connection failed",
            "Connection to database timed out",
            "PostgreSQL connection error",
            "Table 'conversations' doesn't exist",
            "Database is locked"
        ]
        
        for error_message in test_cases:
            response = error_service.analyze_error(error_message=error_message)
            
            assert response.category == ErrorCategory.DATABASE_ERROR
            assert response.severity in [ErrorSeverity.HIGH, ErrorSeverity.CRITICAL]
            assert "Database" in response.title
            assert response.contact_admin is True
            assert any("admin" in step.lower() for step in response.next_steps)

    def test_provider_unavailable_classification_accuracy(self, error_service):
        """Test accurate classification of provider unavailable errors."""
        test_cases = [
            ("Service unavailable", 503, "OpenAI"),
            ("Server temporarily unavailable", 503, "Anthropic"),
            ("503 Service Unavailable", 503, "OpenAI"),
            ("Backend service down", 503, "Anthropic")
        ]
        
        for error_message, status_code, provider in test_cases:
            response = error_service.analyze_error(
                error_message=error_message,
                status_code=status_code,
                provider_name=provider
            )
            
            assert response.category == ErrorCategory.PROVIDER_DOWN
            assert response.severity == ErrorSeverity.HIGH
            assert "Unavailable" in response.title or "Down" in response.title
            assert response.retry_after is not None
            assert any("try again" in step.lower() for step in response.next_steps)

    def test_network_error_classification_accuracy(self, error_service):
        """Test accurate classification of network errors."""
        test_cases = [
            ("Connection timeout", 504),
            ("Request timeout", 408),
            ("Network unreachable", 503),
            ("DNS resolution failed", 503),
            ("Connection refused", 503)
        ]
        
        for error_message, status_code in test_cases:
            response = error_service.analyze_error(
                error_message=error_message,
                status_code=status_code
            )
            
            assert response.category in [ErrorCategory.NETWORK_ERROR, ErrorCategory.PROVIDER_DOWN]
            assert response.severity in [ErrorSeverity.MEDIUM, ErrorSeverity.HIGH]
            assert any("connection" in step.lower() or "network" in step.lower() 
                     for step in response.next_steps)

    def test_validation_error_classification_accuracy(self, error_service):
        """Test accurate classification of validation errors."""
        test_cases = [
            ("Validation failed: required field missing", 400),
            ("Invalid input format", 400),
            ("Missing required parameter", 400),
            ("Field validation error", 422),
            ("Invalid JSON format", 400)
        ]
        
        for error_message, status_code in test_cases:
            response = error_service.analyze_error(
                error_message=error_message,
                status_code=status_code
            )
            
            assert response.category == ErrorCategory.VALIDATION_ERROR
            assert response.severity == ErrorSeverity.LOW
            assert "Invalid" in response.title or "Validation" in response.title
            assert any("check" in step.lower() or "verify" in step.lower() 
                     for step in response.next_steps)
            assert response.contact_admin is False


class TestResponseQuality:
    """Test quality of generated responses."""

    def test_response_completeness(self, error_service):
        """Test that responses contain all required fields."""
        response = error_service.analyze_error(
            error_message="OPENAI_API_KEY not set",
            provider_name="OpenAI"
        )
        
        # Required fields
        assert response.title is not None and len(response.title) > 0
        assert response.summary is not None and len(response.summary) > 0
        assert response.category is not None
        assert response.severity is not None
        assert response.next_steps is not None and len(response.next_steps) > 0
        
        # Optional fields should be properly set when applicable
        assert isinstance(response.contact_admin, bool)
        assert response.retry_after is None or isinstance(response.retry_after, int)
        assert response.help_url is None or isinstance(response.help_url, str)

    def test_next_steps_quality(self, error_service):
        """Test quality and actionability of next steps."""
        test_cases = [
            ("OPENAI_API_KEY not set", "OpenAI"),
            ("Rate limit exceeded", "OpenAI"),
            ("Token has expired", None),
            ("Database connection failed", None)
        ]
        
        for error_message, provider in test_cases:
            response = error_service.analyze_error(
                error_message=error_message,
                provider_name=provider
            )
            
            # Should have 2-4 next steps
            assert 1 <= len(response.next_steps) <= 4
            
            # Each step should be actionable (contain action verbs)
            action_verbs = ["add", "set", "check", "verify", "contact", "try", "wait", "click", "restart"]
            for step in response.next_steps:
                assert len(step) > 10  # Not too short
                assert len(step) < 200  # Not too long
                assert any(verb in step.lower() for verb in action_verbs)
                assert step.endswith(('.', '!', '?'))  # Proper punctuation

    def test_response_tone_consistency(self, error_service):
        """Test that responses maintain consistent, helpful tone."""
        test_cases = [
            "OPENAI_API_KEY not set",
            "Rate limit exceeded", 
            "Token has expired",
            "Database connection failed",
            "Invalid input format"
        ]
        
        # Words that should NOT appear (too technical or harsh)
        forbidden_words = ["error", "failed", "broken", "crash", "bug", "exception"]
        
        # Words that SHOULD appear (helpful and user-friendly)
        helpful_words = ["please", "try", "check", "verify", "contact", "help"]
        
        for error_message in test_cases:
            response = error_service.analyze_error(error_message=error_message)
            
            # Check title and summary tone
            combined_text = f"{response.title} {response.summary}".lower()
            
            # Should avoid harsh technical terms in user-facing text
            harsh_count = sum(1 for word in forbidden_words if word in combined_text)
            assert harsh_count <= 1  # Allow minimal technical terms
            
            # Should include helpful language
            helpful_count = sum(1 for word in helpful_words if word in combined_text)
            assert helpful_count >= 1

    def test_response_specificity(self, error_service):
        """Test that responses are specific and not generic."""
        # Test OpenAI specific response
        openai_response = error_service.analyze_error(
            error_message="OPENAI_API_KEY not set",
            provider_name="OpenAI"
        )
        
        assert "OpenAI" in openai_response.title
        assert "OPENAI_API_KEY" in " ".join(openai_response.next_steps)
        
        # Test Anthropic specific response
        anthropic_response = error_service.analyze_error(
            error_message="ANTHROPIC_API_KEY not found",
            provider_name="Anthropic"
        )
        
        assert "Anthropic" in anthropic_response.title
        assert "ANTHROPIC_API_KEY" in " ".join(anthropic_response.next_steps)
        
        # Responses should be different
        assert openai_response.title != anthropic_response.title
        assert openai_response.next_steps != anthropic_response.next_steps

    def test_help_url_relevance(self, error_service):
        """Test that help URLs are relevant and properly formatted."""
        test_cases = [
            ("OPENAI_API_KEY not set", "OpenAI"),
            ("Rate limit exceeded", "OpenAI"),
            ("Token has expired", None)
        ]
        
        for error_message, provider in test_cases:
            response = error_service.analyze_error(
                error_message=error_message,
                provider_name=provider
            )
            
            if response.help_url:
                # Should be a valid URL format
                assert response.help_url.startswith(('http://', 'https://'))
                assert '.' in response.help_url
                
                # Should be relevant to the error type
                if response.category == ErrorCategory.API_KEY_MISSING:
                    assert any(keyword in response.help_url.lower() 
                             for keyword in ['api', 'key', 'setup', 'config'])


class TestProviderHealthIntegration:
    """Test integration with provider health monitoring."""

    def test_provider_health_context_integration(self, error_service, health_monitor):
        """Test that provider health context is properly integrated."""
        # Mock healthy provider
        health_monitor.get_provider_health.return_value = {
            "status": "healthy",
            "response_time": 0.5,
            "success_rate": 0.95,
            "last_check": datetime.now(timezone.utc).isoformat()
        }
        
        response = error_service.analyze_error(
            error_message="Rate limit exceeded",
            provider_name="OpenAI"
        )
        
        assert response.provider_health is not None
        assert response.provider_health["status"] == "healthy"
        
        # Should suggest waiting rather than switching providers
        next_steps_text = " ".join(response.next_steps).lower()
        assert "wait" in next_steps_text
        assert "alternative" not in next_steps_text

    def test_unhealthy_provider_recommendations(self, error_service, health_monitor):
        """Test recommendations when provider is unhealthy."""
        # Mock unhealthy provider
        health_monitor.get_provider_health.return_value = {
            "status": "unhealthy",
            "response_time": 5.0,
            "success_rate": 0.3,
            "last_check": datetime.now(timezone.utc).isoformat(),
            "consecutive_failures": 5
        }
        
        response = error_service.analyze_error(
            error_message="Service unavailable",
            provider_name="OpenAI"
        )
        
        assert response.provider_health is not None
        assert response.provider_health["status"] == "unhealthy"
        
        # Should suggest alternative providers
        next_steps_text = " ".join(response.next_steps).lower()
        assert any(keyword in next_steps_text 
                  for keyword in ["alternative", "different", "another", "anthropic"])

    def test_provider_health_caching(self, error_service, health_monitor):
        """Test that provider health data is properly cached."""
        # Mock provider health
        health_monitor.get_provider_health.return_value = {
            "status": "healthy",
            "response_time": 0.8,
            "success_rate": 0.92
        }
        
        # Make multiple requests
        for _ in range(3):
            response = error_service.analyze_error(
                error_message="Rate limit exceeded",
                provider_name="OpenAI"
            )
            assert response.provider_health["status"] == "healthy"
        
        # Health monitor should be called only once due to caching
        assert health_monitor.get_provider_health.call_count <= 2


class TestAIOrchestrationIntegration:
    """Test integration with AI orchestrator for enhanced responses."""

    @pytest.mark.asyncio
    async def test_ai_enhanced_response_generation(self, error_service, ai_orchestrator):
        """Test AI-enhanced response generation for complex errors."""
        # Mock AI orchestrator response
        ai_orchestrator.generate_response.return_value = {
            "enhanced_summary": "Your OpenAI API key configuration needs attention",
            "contextual_steps": [
                "Navigate to your .env file in the project root",
                "Add the line: OPENAI_API_KEY=your_actual_key_here",
                "Get your API key from https://platform.openai.com/api-keys",
                "Restart the application to load the new configuration"
            ],
            "confidence": 0.95
        }
        
        response = error_service.analyze_error(
            error_message="OPENAI_API_KEY not set in environment",
            provider_name="OpenAI",
            use_ai_enhancement=True
        )
        
        # Should use AI-enhanced content
        assert "configuration needs attention" in response.summary
        assert len(response.next_steps) == 4
        assert "Navigate to your .env file" in response.next_steps[0]
        
        # Verify AI orchestrator was called
        ai_orchestrator.generate_response.assert_called_once()

    @pytest.mark.asyncio
    async def test_ai_fallback_on_failure(self, error_service, ai_orchestrator):
        """Test fallback to rule-based response when AI fails."""
        # Mock AI orchestrator failure
        ai_orchestrator.generate_response.side_effect = Exception("AI service unavailable")
        
        response = error_service.analyze_error(
            error_message="OPENAI_API_KEY not set",
            provider_name="OpenAI",
            use_ai_enhancement=True
        )
        
        # Should still provide a valid response using rules
        assert response.title is not None
        assert response.summary is not None
        assert len(response.next_steps) > 0
        assert response.category == ErrorCategory.API_KEY_MISSING

    @pytest.mark.asyncio
    async def test_ai_response_quality_validation(self, error_service, ai_orchestrator):
        """Test validation of AI-generated responses."""
        # Mock low-quality AI response
        ai_orchestrator.generate_response.return_value = {
            "enhanced_summary": "Error occurred",  # Too generic
            "contextual_steps": ["Fix it"],  # Too vague
            "confidence": 0.3  # Low confidence
        }
        
        response = error_service.analyze_error(
            error_message="OPENAI_API_KEY not set",
            provider_name="OpenAI",
            use_ai_enhancement=True
        )
        
        # Should fall back to rule-based response due to low quality
        assert "OpenAI API Key" in response.title
        assert "OPENAI_API_KEY" in " ".join(response.next_steps)
        assert len(response.next_steps) > 1


class TestResponseConsistency:
    """Test consistency of responses across multiple calls."""

    def test_deterministic_classification(self, error_service):
        """Test that same errors produce consistent classifications."""
        error_message = "OPENAI_API_KEY not set in environment"
        
        responses = []
        for _ in range(5):
            response = error_service.analyze_error(
                error_message=error_message,
                provider_name="OpenAI"
            )
            responses.append(response)
        
        # All responses should have same classification
        categories = [r.category for r in responses]
        severities = [r.severity for r in responses]
        
        assert len(set(categories)) == 1  # All same category
        assert len(set(severities)) == 1  # All same severity
        assert all(r.category == ErrorCategory.API_KEY_MISSING for r in responses)

    def test_consistent_next_steps_ordering(self, error_service):
        """Test that next steps are consistently ordered by priority."""
        responses = []
        for _ in range(3):
            response = error_service.analyze_error(
                error_message="OPENAI_API_KEY not set",
                provider_name="OpenAI"
            )
            responses.append(response)
        
        # First step should always be the most important (adding the key)
        for response in responses:
            first_step = response.next_steps[0].lower()
            assert "add" in first_step and "openai_api_key" in first_step

    def test_response_stability_over_time(self, error_service):
        """Test that responses remain stable over multiple calls."""
        error_message = "Rate limit exceeded for requests"
        
        # Collect responses over simulated time
        responses = []
        for i in range(10):
            response = error_service.analyze_error(
                error_message=error_message,
                status_code=429,
                provider_name="OpenAI"
            )
            responses.append(response)
        
        # Core response elements should be stable
        titles = [r.title for r in responses]
        categories = [r.category for r in responses]
        
        assert len(set(titles)) == 1  # Same title
        assert len(set(categories)) == 1  # Same category
        assert all(r.category == ErrorCategory.RATE_LIMIT for r in responses)


class TestEdgeCases:
    """Test edge cases and boundary conditions."""

    def test_empty_error_message(self, error_service):
        """Test handling of empty error messages."""
        response = error_service.analyze_error(error_message="")
        
        assert response.category == ErrorCategory.UNKNOWN
        assert response.title is not None
        assert len(response.next_steps) > 0
        assert response.contact_admin is True

    def test_very_long_error_message(self, error_service):
        """Test handling of very long error messages."""
        long_message = "Error: " + "x" * 10000  # Very long error
        
        response = error_service.analyze_error(error_message=long_message)
        
        assert response.title is not None
        assert len(response.title) < 200  # Title should be reasonable length
        assert len(response.summary) < 500  # Summary should be reasonable length

    def test_special_characters_in_error(self, error_service):
        """Test handling of special characters in error messages."""
        special_message = "Error: <script>alert('xss')</script> & 'quotes' \"double\" \\backslash"
        
        response = error_service.analyze_error(error_message=special_message)
        
        # Should handle special characters gracefully
        assert response.title is not None
        assert response.summary is not None
        assert len(response.next_steps) > 0

    def test_multiple_error_patterns_in_message(self, error_service):
        """Test handling when error message matches multiple patterns."""
        mixed_message = "OPENAI_API_KEY not set and rate limit exceeded"
        
        response = error_service.analyze_error(
            error_message=mixed_message,
            provider_name="OpenAI"
        )
        
        # Should prioritize the most specific/severe error
        assert response.category in [ErrorCategory.API_KEY_MISSING, ErrorCategory.RATE_LIMIT]
        assert response.severity in [ErrorSeverity.HIGH, ErrorSeverity.MEDIUM]

    def test_unknown_provider_handling(self, error_service):
        """Test handling of unknown provider names."""
        response = error_service.analyze_error(
            error_message="API key not found",
            provider_name="UnknownProvider"
        )
        
        # Should still provide helpful response
        assert response.category == ErrorCategory.API_KEY_MISSING
        assert "API" in response.title
        assert len(response.next_steps) > 0


if __name__ == "__main__":
    pytest.main([__file__, "-v"])