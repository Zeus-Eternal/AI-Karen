{
  "id": null,
  "uid": "llamacpp-runtime",
  "title": "Llama-CPP Runtime Operations",
  "tags": [
    "llama-cpp",
    "runtime",
    "kari-ai"
  ],
  "timezone": "browser",
  "schemaVersion": 38,
  "version": 1,
  "refresh": "30s",
  "style": "dark",
  "time": {
    "from": "now-1h",
    "to": "now"
  },
  "timepicker": {
    "refresh_intervals": [
      "5s",
      "10s",
      "30s",
      "1m",
      "5m",
      "15m"
    ],
    "time_options": [
      "last 15 minutes",
      "last 30 minutes",
      "last 1 hour",
      "last 6 hours",
      "last 12 hours",
      "last 24 hours"
    ]
  },
  "templating": {
    "list": [
      {
        "name": "instance",
        "type": "query",
        "datasource": {
          "type": "prometheus",
          "uid": "prometheus"
        },
        "refresh": 2,
        "query": "label_values(chat_response_latency_ms, instance)",
        "includeAll": true,
        "multi": true,
        "current": {
          "text": "All",
          "value": "$__all"
        }
      },
      {
        "name": "model",
        "type": "query",
        "datasource": {
          "type": "prometheus",
          "uid": "prometheus"
        },
        "refresh": 2,
        "query": "label_values(llamacpp_inference_latency_ms, model)",
        "includeAll": true,
        "multi": true,
        "current": {
          "text": "All",
          "value": "$__all"
        }
      }
    ]
  },
  "annotations": {
    "list": [
      {
        "type": "dashboard",
        "name": "Annotations & Alerts",
        "enable": true,
        "hide": false
      }
    ]
  },
  "panels": [
    {
      "title": "Chat Response Latency (p95)",
      "type": "stat",
      "gridPos": {
        "h": 6,
        "w": 6,
        "x": 0,
        "y": 0
      },
      "fieldConfig": {
        "defaults": {
          "unit": "ms",
          "mappings": [],
          "thresholds": {
            "mode": "absolute",
            "steps": [
              {
                "value": null,
                "color": "green"
              },
              {
                "value": 5000,
                "color": "yellow"
              },
              {
                "value": 6000,
                "color": "red"
              }
            ]
          }
        },
        "overrides": []
      },
      "options": {
        "reduceOptions": {
          "calcs": [
            "lastNotNull"
          ],
          "fields": "",
          "values": false
        },
        "orientation": "horizontal",
        "colorMode": "value",
        "graphMode": "area",
        "justifyMode": "auto",
        "textMode": "value"
      },
      "targets": [
        {
          "refId": "A",
          "datasource": {
            "type": "prometheus",
            "uid": "prometheus"
          },
          "expr": "histogram_quantile(0.95, sum(rate(chat_response_latency_ms_bucket{instance=~\"$instance\"}[5m])) by (le))",
          "legendFormat": "p95"
        }
      ]
    },
    {
      "title": "Llama-CPP Inference Latency (p95)",
      "type": "stat",
      "gridPos": {
        "h": 6,
        "w": 6,
        "x": 6,
        "y": 0
      },
      "fieldConfig": {
        "defaults": {
          "unit": "ms",
          "thresholds": {
            "mode": "absolute",
            "steps": [
              {
                "value": null,
                "color": "green"
              },
              {
                "value": 1200,
                "color": "yellow"
              },
              {
                "value": 1500,
                "color": "red"
              }
            ]
          }
        }
      },
      "options": {
        "reduceOptions": {
          "calcs": [
            "lastNotNull"
          ],
          "fields": "",
          "values": false
        },
        "orientation": "horizontal",
        "colorMode": "value",
        "graphMode": "area",
        "justifyMode": "auto",
        "textMode": "value"
      },
      "targets": [
        {
          "refId": "A",
          "datasource": {
            "type": "prometheus",
            "uid": "prometheus"
          },
          "expr": "histogram_quantile(0.95, sum(rate(llamacpp_inference_latency_ms_bucket{instance=~\"$instance\", model=~\"$model\"}[5m])) by (le))",
          "legendFormat": "p95"
        }
      ]
    },
    {
      "title": "Tokens Per Second",
      "type": "timeseries",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 12,
        "y": 0
      },
      "fieldConfig": {
        "defaults": {
          "unit": "ops",
          "color": {
            "mode": "palette-classic"
          }
        },
        "overrides": []
      },
      "options": {
        "legend": {
          "displayMode": "table",
          "placement": "bottom"
        },
        "tooltip": {
          "mode": "single"
        }
      },
      "targets": [
        {
          "refId": "A",
          "datasource": {
            "type": "prometheus",
            "uid": "prometheus"
          },
          "expr": "avg(rate(tokens_per_second{instance=~\"$instance\", model=~\"$model\"}[1m])) by (model)",
          "legendFormat": "{{model}}"
        }
      ]
    },
    {
      "title": "Memory Fetch Latency",
      "type": "timeseries",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 6
      },
      "fieldConfig": {
        "defaults": {
          "unit": "ms",
          "color": {
            "mode": "palette-classic"
          },
          "thresholds": {
            "mode": "absolute",
            "steps": [
              {
                "color": "green",
                "value": null
              },
              {
                "color": "yellow",
                "value": 150
              },
              {
                "color": "red",
                "value": 300
              }
            ]
          }
        }
      },
      "options": {
        "legend": {
          "displayMode": "table",
          "placement": "bottom"
        },
        "tooltip": {
          "mode": "multi"
        }
      },
      "targets": [
        {
          "refId": "A",
          "datasource": {
            "type": "prometheus",
            "uid": "prometheus"
          },
          "expr": "avg(rate(memory_fetch_ms_sum{instance=~\"$instance\"}[5m])) / avg(rate(memory_fetch_ms_count{instance=~\"$instance\"}[5m]))",
          "legendFormat": "Avg Memory Fetch"
        }
      ]
    },
    {
      "title": "Fallback Activation Rate",
      "type": "timeseries",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 12,
        "y": 8
      },
      "fieldConfig": {
        "defaults": {
          "unit": "percent",
          "color": {
            "mode": "continuous-GrYlRd"
          },
          "thresholds": {
            "mode": "percentage",
            "steps": [
              {
                "value": null,
                "color": "green"
              },
              {
                "value": 3,
                "color": "yellow"
              },
              {
                "value": 5,
                "color": "red"
              }
            ]
          }
        }
      },
      "options": {
        "legend": {
          "displayMode": "table",
          "placement": "bottom"
        },
        "tooltip": {
          "mode": "single"
        }
      },
      "targets": [
        {
          "refId": "A",
          "datasource": {
            "type": "prometheus",
            "uid": "prometheus"
          },
          "expr": "(sum(increase(fallback_invocations_total{instance=~\"$instance\"}[5m])) / sum(increase(chat_runtime_requests_total{instance=~\"$instance\"}[5m]))) * 100",
          "legendFormat": "Fallback %"
        }
      ]
    },
    {
      "title": "System Load & GPU Utilization",
      "type": "timeseries",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 14
      },
      "fieldConfig": {
        "defaults": {
          "unit": "percent",
          "color": {
            "mode": "palette-classic"
          }
        }
      },
      "options": {
        "legend": {
          "displayMode": "table",
          "placement": "bottom"
        },
        "tooltip": {
          "mode": "multi"
        }
      },
      "targets": [
        {
          "refId": "A",
          "datasource": {
            "type": "prometheus",
            "uid": "prometheus"
          },
          "expr": "avg(node_load1{instance=~\"$instance\"})",
          "legendFormat": "CPU Load"
        },
        {
          "refId": "B",
          "datasource": {
            "type": "prometheus",
            "uid": "prometheus"
          },
          "expr": "avg(nvidia_gpu_utilization{instance=~\"$instance\"})",
          "legendFormat": "GPU Utilization"
        }
      ]
    },
    {
      "title": "Tokens Used Per Response",
      "type": "timeseries",
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 12,
        "y": 16
      },
      "fieldConfig": {
        "defaults": {
          "unit": "short",
          "color": {
            "mode": "palette-classic"
          }
        }
      },
      "options": {
        "legend": {
          "displayMode": "table",
          "placement": "bottom"
        },
        "tooltip": {
          "mode": "multi"
        }
      },
      "targets": [
        {
          "refId": "A",
          "datasource": {
            "type": "prometheus",
            "uid": "prometheus"
          },
          "expr": "sum(rate(tokens_used_sum{instance=~\"$instance\", model=~\"$model\"}[5m])) / sum(rate(tokens_used_count{instance=~\"$instance\", model=~\"$model\"}[5m]))",
          "legendFormat": "{{model}}"
        }
      ]
    }
  ]
}
