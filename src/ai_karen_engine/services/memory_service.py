"""
Enhanced Memory Service for Web UI Integration.
Extends the existing MemoryManager with web UI specific features and query methods.
"""

import logging
import uuid
from dataclasses import dataclass
from datetime import datetime, timedelta
from enum import Enum
from typing import Any, Dict, List, Optional, Tuple, Union

from pydantic import BaseModel, ConfigDict, Field
from sqlalchemy import and_, desc, select, update

from ai_karen_engine.database.memory_manager import (
    MemoryEntry,
    MemoryManager,
    MemoryQuery,
)
from ai_karen_engine.database.models import TenantConversation, TenantMemoryEntry

logger = logging.getLogger(__name__)


class MemoryType(str, Enum):
    """Types of memory entries for web UI categorization."""

    GENERAL = "general"
    FACT = "fact"
    PREFERENCE = "preference"
    CONTEXT = "context"
    CONVERSATION = "conversation"
    INSIGHT = "insight"


class UISource(str, Enum):
    """Source UI types for tracking memory origin."""

    WEB = "web"
    DESKTOP = "desktop"
    API = "api"
    AG_UI = "ag_ui"


@dataclass
class WebUIMemoryEntry(MemoryEntry):
    """Extended memory entry with web UI specific fields."""

    ui_source: Optional[UISource] = None
    conversation_id: Optional[str] = None
    memory_type: MemoryType = MemoryType.GENERAL
    importance_score: int = 5
    access_count: int = 0
    last_accessed: Optional[datetime] = None
    ai_generated: bool = False
    user_confirmed: bool = True

    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary with web UI fields."""
        base_dict = super().to_dict()
        base_dict.update(
            {
                "ui_source": self.ui_source.value if self.ui_source else None,
                "conversation_id": self.conversation_id,
                "memory_type": self.memory_type.value,
                "importance_score": self.importance_score,
                "access_count": self.access_count,
                "last_accessed": self.last_accessed.isoformat()
                if self.last_accessed
                else None,
                "ai_generated": self.ai_generated,
                "user_confirmed": self.user_confirmed,
            }
        )
        return base_dict


class WebUIMemoryQuery(BaseModel):
    """Enhanced memory query with web UI specific parameters."""

    text: str
    user_id: Optional[str] = None
    session_id: Optional[str] = None
    conversation_id: Optional[str] = None
    ui_source: Optional[UISource] = None
    memory_types: List[MemoryType] = Field(default_factory=list)
    tags: List[str] = Field(default_factory=list)
    importance_range: Optional[Tuple[int, int]] = None
    only_user_confirmed: bool = True
    only_ai_generated: bool = False
    time_range: Optional[Tuple[datetime, datetime]] = None
    top_k: int = 10
    similarity_threshold: float = 0.7
    include_embeddings: bool = False

    def to_memory_query(self) -> MemoryQuery:
        """Convert to base MemoryQuery for compatibility."""
        return MemoryQuery(
            text=self.text,
            user_id=self.user_id,
            session_id=self.session_id,
            conversation_id=self.conversation_id,
            tags=self.tags,
            time_range=self.time_range,
            top_k=self.top_k,
            similarity_threshold=self.similarity_threshold,
            include_embeddings=self.include_embeddings,
        )


class MemoryContextBuilder:
    """Builds conversation context from relevant memories."""

    def __init__(self, memory_service: "WebUIMemoryService"):
        self.memory_service = memory_service
        self.max_context_tokens = 2000  # Approximate token limit for context
        self.context_weights = {
            MemoryType.FACT: 1.0,
            MemoryType.PREFERENCE: 0.9,
            MemoryType.CONTEXT: 0.8,
            MemoryType.CONVERSATION: 0.7,
            MemoryType.INSIGHT: 0.6,
            MemoryType.GENERAL: 0.5,
        }

    async def build_context(
        self,
        tenant_id: Union[str, uuid.UUID],
        query: str,
        user_id: str,
        session_id: Optional[str] = None,
        conversation_id: Optional[str] = None,
    ) -> Dict[str, Any]:
        """Build conversation context from relevant memories using NLP-enhanced retrieval."""
        try:
            # Use NLP to analyze the query for better context building
            query_analysis = await self._analyze_query_with_nlp(query)

            # Query for relevant memories with different types
            memory_query = WebUIMemoryQuery(
                text=query,
                user_id=user_id,
                session_id=session_id,
                conversation_id=conversation_id,
                top_k=20,  # Get more memories for context building
                similarity_threshold=0.6,  # Lower threshold for context
            )

            memories = await self.memory_service.query_memories(tenant_id, memory_query)

            # Enhance memory ranking using NLP analysis
            memories = await self._enhance_memory_ranking_with_nlp(
                memories, query_analysis
            )

            # Group memories by type
            memories_by_type = {}
            for memory in memories:
                mem_type = memory.memory_type
                if mem_type not in memories_by_type:
                    memories_by_type[mem_type] = []
                memories_by_type[mem_type].append(memory)

            # Build weighted context
            context_parts = []
            total_tokens = 0

            # Process memories by importance (facts first, then preferences, etc.)
            for mem_type in sorted(
                self.context_weights.keys(),
                key=lambda x: self.context_weights[x],
                reverse=True,
            ):
                if mem_type not in memories_by_type:
                    continue

                type_memories = memories_by_type[mem_type]
                # Sort by enhanced importance score and similarity
                type_memories.sort(
                    key=lambda m: (
                        getattr(m, "nlp_relevance_score", m.importance_score),
                        m.similarity_score or 0,
                    ),
                    reverse=True,
                )

                for memory in type_memories:
                    # Estimate tokens (rough approximation: 1 token â‰ˆ 4 characters)
                    memory_tokens = len(memory.content) // 4
                    if total_tokens + memory_tokens > self.max_context_tokens:
                        break

                    context_parts.append(
                        {
                            "type": mem_type.value,
                            "content": memory.content,
                            "importance": memory.importance_score,
                            "similarity": memory.similarity_score,
                            "nlp_relevance": getattr(
                                memory, "nlp_relevance_score", None
                            ),
                            "timestamp": memory.timestamp,
                            "tags": memory.tags,
                        }
                    )
                    total_tokens += memory_tokens

                if total_tokens >= self.max_context_tokens:
                    break

            # Get conversation history if conversation_id provided
            conversation_context = None
            if conversation_id:
                conversation_context = await self._get_conversation_context(
                    tenant_id, conversation_id
                )

            return {
                "memories": context_parts,
                "total_memories": len(context_parts),
                "memory_types_found": list(memories_by_type.keys()),
                "conversation_context": conversation_context,
                "query_analysis": query_analysis,
                "context_metadata": {
                    "query": query,
                    "user_id": user_id,
                    "session_id": session_id,
                    "conversation_id": conversation_id,
                    "total_tokens_estimate": total_tokens,
                    "generated_at": datetime.utcnow().isoformat(),
                },
            }

        except Exception as e:
            logger.error(f"Failed to build context for tenant {tenant_id}: {e}")
            return {
                "memories": [],
                "total_memories": 0,
                "memory_types_found": [],
                "conversation_context": None,
                "error": str(e),
            }

    async def _analyze_query_with_nlp(self, query: str) -> Dict[str, Any]:
        """Analyze query using NLP to extract key information for better context building."""
        try:
            # Import spaCy service for query analysis
            from ai_karen_engine.services.nlp_service_manager import nlp_service_manager

            # Get comprehensive NLP analysis of the query
            parsed_query = await nlp_service_manager.spacy_service.parse_message(query)
            linguistic_features = (
                await nlp_service_manager.spacy_service.get_linguistic_features(query)
            )

            # Extract key entities and their types
            key_entities = []
            for entity_text, entity_label in parsed_query.entities:
                key_entities.append(
                    {
                        "text": entity_text,
                        "label": entity_label,
                        "importance": 1.0
                        if entity_label in ["PERSON", "ORG", "GPE"]
                        else 0.8,
                    }
                )

            # Extract important nouns and verbs
            important_tokens = []
            for token, pos in parsed_query.pos_tags:
                if pos in ["NOUN", "PROPN", "VERB"] and len(token) > 2:
                    important_tokens.append(
                        {
                            "text": token,
                            "pos": pos,
                            "importance": 1.0 if pos in ["PROPN", "NOUN"] else 0.7,
                        }
                    )

            return {
                "entities": key_entities,
                "important_tokens": important_tokens,
                "sentences": parsed_query.sentences,
                "noun_phrases": parsed_query.noun_phrases,
                "linguistic_features": linguistic_features,
                "query_complexity": len(parsed_query.dependencies),
                "used_fallback": parsed_query.used_fallback,
            }

        except Exception as e:
            logger.warning(f"Failed to analyze query with NLP: {e}")
            # Return basic analysis as fallback
            return {
                "entities": [],
                "important_tokens": [],
                "sentences": [query],
                "noun_phrases": [],
                "linguistic_features": {},
                "query_complexity": 0,
                "used_fallback": True,
            }

    async def _enhance_memory_ranking_with_nlp(
        self, memories: List["WebUIMemoryEntry"], query_analysis: Dict[str, Any]
    ) -> List["WebUIMemoryEntry"]:
        """Enhance memory ranking using NLP analysis of query and memory content."""
        try:
            # Import spaCy service for memory analysis
            from ai_karen_engine.services.nlp_service_manager import nlp_service_manager

            query_entities = {
                entity["text"].lower() for entity in query_analysis.get("entities", [])
            }
            query_tokens = {
                token["text"].lower()
                for token in query_analysis.get("important_tokens", [])
            }

            enhanced_memories = []
            for memory in memories:
                try:
                    # Analyze memory content
                    memory_entities = (
                        await nlp_service_manager.spacy_service.extract_entities(
                            memory.content
                        )
                    )
                    memory_entity_texts = {
                        entity[0].lower() for entity in memory_entities
                    }

                    # Calculate NLP-based relevance score
                    entity_overlap = len(
                        query_entities.intersection(memory_entity_texts)
                    )

                    # Simple token overlap (could be enhanced with semantic similarity)
                    memory_tokens = set(memory.content.lower().split())
                    token_overlap = len(query_tokens.intersection(memory_tokens))

                    # Calculate enhanced relevance score
                    base_score = memory.similarity_score or 0.0
                    entity_boost = entity_overlap * 0.2  # Boost for entity matches
                    token_boost = token_overlap * 0.1  # Boost for token matches

                    nlp_relevance_score = base_score + entity_boost + token_boost

                    # Add the enhanced score as an attribute
                    memory.nlp_relevance_score = min(
                        nlp_relevance_score, 1.0
                    )  # Cap at 1.0
                    enhanced_memories.append(memory)

                except Exception as e:
                    logger.warning(
                        f"Failed to enhance ranking for memory {memory.id}: {e}"
                    )
                    # Keep original memory without enhancement
                    memory.nlp_relevance_score = memory.similarity_score or 0.0
                    enhanced_memories.append(memory)

            return enhanced_memories

        except Exception as e:
            logger.warning(f"Failed to enhance memory ranking with NLP: {e}")
            # Return original memories without enhancement
            for memory in memories:
                memory.nlp_relevance_score = memory.similarity_score or 0.0
            return memories

    async def _get_conversation_context(
        self, tenant_id: Union[str, uuid.UUID], conversation_id: str
    ) -> Optional[Dict[str, Any]]:
        """Get conversation context from database."""
        try:
            async with self.memory_service.db_client.get_async_session() as session:
                result = await session.execute(
                    select(TenantConversation).where(
                        TenantConversation.id == uuid.UUID(conversation_id)
                    )
                )
                conversation = result.scalar_one_or_none()

                if conversation:
                    return {
                        "title": conversation.title,
                        "summary": conversation.summary,
                        "ui_context": conversation.ui_context,
                        "ai_insights": conversation.ai_insights,
                        "user_settings": conversation.user_settings,
                        "tags": conversation.tags,
                        "last_updated": conversation.updated_at.isoformat(),
                    }

                return None

        except Exception as e:
            logger.error(f"Failed to get conversation context: {e}")
            return None


class WebUIMemoryService:
    """Enhanced memory service with web UI specific features."""

    def __init__(self, base_memory_manager: Optional[MemoryManager]):
        """Initialize with existing memory manager."""
        self.base_manager = base_memory_manager
        self.db_client = base_memory_manager.db_client if base_memory_manager else None
        self.context_builder = MemoryContextBuilder(self)

        # Web UI specific configuration
        self.default_importance_score = 5
        self.auto_tag_enabled = True
        self.fact_extraction_enabled = True

        # Performance metrics
        self.web_ui_metrics = {
            "context_builds": 0,
            "web_ui_queries": 0,
            "memory_confirmations": 0,
            "auto_tags_generated": 0,
            "facts_extracted": 0,
        }

    async def store_web_ui_memory(
        self,
        tenant_id: Union[str, uuid.UUID],
        content: str,
        user_id: str,
        ui_source: UISource,
        session_id: Optional[str] = None,
        conversation_id: Optional[str] = None,
        memory_type: MemoryType = MemoryType.GENERAL,
        tags: Optional[List[str]] = None,
        importance_score: int = None,
        ai_generated: bool = False,
        metadata: Optional[Dict[str, Any]] = None,
        ttl_hours: Optional[int] = None,
        tenant_filters: Optional[Dict[str, Any]] = None,
    ) -> Optional[str]:
        """Store memory with web UI specific features."""
        if not self.base_manager:
            logger.warning("Memory manager unavailable - skipping memory store")
            return None
        try:
            # Auto-generate tags if enabled
            if self.auto_tag_enabled and not tags:
                tags = await self._generate_auto_tags(content, memory_type)
                if tags:
                    self.web_ui_metrics["auto_tags_generated"] += 1

            # Extract facts if enabled and memory type is appropriate
            extracted_facts = []
            if (
                self.fact_extraction_enabled
                and memory_type in [MemoryType.FACT, MemoryType.GENERAL]
                and not ai_generated
            ):
                extracted_facts = await self._extract_facts(content)
                if extracted_facts:
                    self.web_ui_metrics["facts_extracted"] += 1

            # Prepare metadata with web UI fields
            web_ui_metadata = {
                "ui_source": ui_source.value,
                "conversation_id": conversation_id,
                "memory_type": memory_type.value,
                "importance_score": importance_score or self.default_importance_score,
                "ai_generated": ai_generated,
                "user_confirmed": not ai_generated,  # AI generated memories need confirmation
                "extracted_facts": extracted_facts,
                "access_count": 0,
            }

            if metadata:
                web_ui_metadata.update(metadata)

            # Store using base manager with correct parameters
            # Include user_id, session_id, tags, and ttl_hours in metadata
            web_ui_metadata.update(
                {
                    "user_id": user_id,
                    "session_id": session_id,
                    "tags": tags or [],
                    "ttl_hours": ttl_hours,
                }
            )

            memory_id = await self.base_manager.store_memory(
                tenant_id=tenant_id,
                content=content,
                scope=f"user:{user_id}",  # Use user_id as scope
                kind=memory_type.value,  # Use memory_type as kind
                metadata=web_ui_metadata,
            )

            # Update database with web UI specific fields
            if memory_id:
                await self._update_web_ui_fields(tenant_id, memory_id, web_ui_metadata)

            return memory_id

        except Exception as e:
            logger.error(f"Failed to store web UI memory: {e}")
            raise

    async def query_memories(
        self,
        tenant_id: Union[str, uuid.UUID],
        query: WebUIMemoryQuery,
        tenant_filters: Optional[Dict[str, Any]] = None,
    ) -> List[WebUIMemoryEntry]:
        """Query memories with web UI specific filtering."""
        if not self.base_manager:
            logger.warning("Memory manager unavailable - returning empty results")
            return []
        try:
            self.web_ui_metrics["web_ui_queries"] += 1

            # Convert to base query for vector search
            base_query = query.to_memory_query()
            base_memories = await self.base_manager.query_memories(
                tenant_id, base_query
            )

            # Get additional web UI data from database
            memory_ids = [m.id for m in base_memories]
            web_ui_data = await self._get_web_ui_memory_data(tenant_id, memory_ids)

            # Convert to WebUIMemoryEntry with filtering
            web_ui_memories = []
            for base_memory in base_memories:
                web_data = web_ui_data.get(base_memory.id, {})

                # Apply web UI specific filters
                if not self._passes_web_ui_filters(web_data, query):
                    continue

                # Update access count
                await self._increment_access_count(tenant_id, base_memory.id)

                web_ui_memory = WebUIMemoryEntry(
                    id=base_memory.id,
                    content=base_memory.content,
                    embedding=base_memory.embedding,
                    metadata=base_memory.metadata,
                    timestamp=base_memory.timestamp,
                    ttl=base_memory.ttl,
                    user_id=base_memory.user_id,
                    session_id=base_memory.session_id,
                    tags=base_memory.tags,
                    similarity_score=base_memory.similarity_score,
                    # Web UI specific fields
                    ui_source=UISource(web_data.get("ui_source", "api")),
                    conversation_id=web_data.get("conversation_id"),
                    memory_type=MemoryType(web_data.get("memory_type", "general")),
                    importance_score=web_data.get("importance_score", 5),
                    access_count=web_data.get("access_count", 0),
                    last_accessed=web_data.get("last_accessed"),
                    ai_generated=web_data.get("ai_generated", False),
                    user_confirmed=web_data.get("user_confirmed", True),
                )

                web_ui_memories.append(web_ui_memory)

            return web_ui_memories

        except Exception as e:
            logger.error(f"Failed to query web UI memories: {e}")
            raise

    async def build_conversation_context(
        self,
        tenant_id: Union[str, uuid.UUID],
        query: str,
        user_id: str,
        session_id: Optional[str] = None,
        conversation_id: Optional[str] = None,
    ) -> Dict[str, Any]:
        """Build conversation context from relevant memories."""
        self.web_ui_metrics["context_builds"] += 1
        return await self.context_builder.build_context(
            tenant_id, query, user_id, session_id, conversation_id
        )

    async def confirm_memory(
        self, tenant_id: Union[str, uuid.UUID], memory_id: str, confirmed: bool = True
    ) -> bool:
        """Confirm or reject an AI-generated memory."""
        try:
            async with self.db_client.get_async_session() as session:
                await session.execute(
                    update(TenantMemoryEntry)
                    .where(TenantMemoryEntry.vector_id == memory_id)
                    .values(user_confirmed=confirmed)
                )
                await session.commit()

                if confirmed:
                    self.web_ui_metrics["memory_confirmations"] += 1

                return True

        except Exception as e:
            logger.error(f"Failed to confirm memory {memory_id}: {e}")
            return False

    async def update_memory_importance(
        self, tenant_id: Union[str, uuid.UUID], memory_id: str, importance_score: int
    ) -> bool:
        """Update memory importance score."""
        if not (1 <= importance_score <= 10):
            raise ValueError("Importance score must be between 1 and 10")

        try:
            async with self.db_client.get_async_session() as session:
                await session.execute(
                    update(TenantMemoryEntry)
                    .where(TenantMemoryEntry.vector_id == memory_id)
                    .values(importance_score=importance_score)
                )
                await session.commit()
                return True

        except Exception as e:
            logger.error(f"Failed to update memory importance: {e}")
            return False

    async def get_memory_analytics(
        self,
        tenant_id: Union[str, uuid.UUID],
        user_id: Optional[str] = None,
        time_range: Optional[Tuple[datetime, datetime]] = None,
    ) -> Dict[str, Any]:
        """Get memory analytics for web UI dashboard."""
        try:
            async with self.db_client.get_async_session() as session:
                # Base query
                query_conditions = []
                if user_id:
                    query_conditions.append(
                        TenantMemoryEntry.user_id == uuid.UUID(user_id)
                    )
                if time_range:
                    query_conditions.append(
                        TenantMemoryEntry.created_at >= time_range[0]
                    )
                    query_conditions.append(
                        TenantMemoryEntry.created_at <= time_range[1]
                    )

                base_query = select(TenantMemoryEntry)
                if query_conditions:
                    base_query = base_query.where(and_(*query_conditions))

                # Get all memories for analysis
                result = await session.execute(base_query)
                memories = result.fetchall()

                # Analyze memories
                analytics = {
                    "total_memories": len(memories),
                    "memories_by_type": {},
                    "memories_by_ui_source": {},
                    "memories_by_importance": {},
                    "ai_generated_count": 0,
                    "user_confirmed_count": 0,
                    "average_importance": 0,
                    "most_accessed_memories": [],
                    "recent_activity": [],
                    "tag_frequency": {},
                    "web_ui_metrics": self.web_ui_metrics.copy(),
                }

                if not memories:
                    return analytics

                # Process memories
                importance_sum = 0
                for memory in memories:
                    # Memory type distribution
                    mem_type = memory.memory_type or "general"
                    analytics["memories_by_type"][mem_type] = (
                        analytics["memories_by_type"].get(mem_type, 0) + 1
                    )

                    # UI source distribution
                    ui_source = memory.ui_source or "unknown"
                    analytics["memories_by_ui_source"][ui_source] = (
                        analytics["memories_by_ui_source"].get(ui_source, 0) + 1
                    )

                    # Importance distribution
                    importance = memory.importance_score or 5
                    analytics["memories_by_importance"][str(importance)] = (
                        analytics["memories_by_importance"].get(str(importance), 0) + 1
                    )
                    importance_sum += importance

                    # AI generated vs user confirmed
                    if memory.ai_generated:
                        analytics["ai_generated_count"] += 1
                    if memory.user_confirmed:
                        analytics["user_confirmed_count"] += 1

                    # Tag frequency
                    if memory.tags:
                        for tag in memory.tags:
                            analytics["tag_frequency"][tag] = (
                                analytics["tag_frequency"].get(tag, 0) + 1
                            )

                analytics["average_importance"] = importance_sum / len(memories)

                # Get most accessed memories
                most_accessed_query = base_query.order_by(
                    desc(TenantMemoryEntry.access_count)
                ).limit(10)
                most_accessed_result = await session.execute(most_accessed_query)
                analytics["most_accessed_memories"] = [
                    {
                        "id": mem.vector_id,
                        "content": mem.content[:100] + "..."
                        if len(mem.content) > 100
                        else mem.content,
                        "access_count": mem.access_count,
                        "importance_score": mem.importance_score,
                    }
                    for mem in most_accessed_result.fetchall()
                ]

                # Recent activity (last 24 hours)
                recent_cutoff = datetime.utcnow() - timedelta(hours=24)
                recent_query = base_query.where(
                    TenantMemoryEntry.created_at > recent_cutoff
                )
                recent_result = await session.execute(recent_query)
                analytics["recent_activity"] = [
                    {
                        "id": mem.vector_id,
                        "content": mem.content[:50] + "..."
                        if len(mem.content) > 50
                        else mem.content,
                        "memory_type": mem.memory_type,
                        "created_at": mem.created_at.isoformat(),
                    }
                    for mem in recent_result.fetchall()
                ]

                return analytics

        except Exception as e:
            logger.error(f"Failed to get memory analytics: {e}")
            return {"error": str(e)}

    async def _update_web_ui_fields(
        self,
        tenant_id: Union[str, uuid.UUID],
        memory_id: str,
        web_ui_metadata: Dict[str, Any],
    ):
        """Update database with web UI specific fields."""
        try:
            async with self.db_client.get_async_session() as session:
                await session.execute(
                    update(TenantMemoryEntry)
                    .where(TenantMemoryEntry.vector_id == memory_id)
                    .values(
                        ui_source=web_ui_metadata.get("ui_source"),
                        conversation_id=uuid.UUID(web_ui_metadata["conversation_id"])
                        if web_ui_metadata.get("conversation_id")
                        else None,
                        memory_type=web_ui_metadata.get("memory_type"),
                        importance_score=web_ui_metadata.get("importance_score"),
                        ai_generated=web_ui_metadata.get("ai_generated"),
                        user_confirmed=web_ui_metadata.get("user_confirmed"),
                    )
                )
                await session.commit()

        except Exception as e:
            logger.error(f"Failed to update web UI fields: {e}")

    async def _get_web_ui_memory_data(
        self, tenant_id: Union[str, uuid.UUID], memory_ids: List[str]
    ) -> Dict[str, Dict[str, Any]]:
        """Get web UI specific data for memory entries."""
        try:
            async with self.db_client.get_async_session() as session:
                result = await session.execute(
                    select(TenantMemoryEntry).where(
                        TenantMemoryEntry.vector_id.in_(memory_ids)
                    )
                )

                web_ui_data = {}
                for memory in result.fetchall():
                    web_ui_data[memory.vector_id] = {
                        "ui_source": memory.ui_source,
                        "conversation_id": str(memory.conversation_id)
                        if memory.conversation_id
                        else None,
                        "memory_type": memory.memory_type,
                        "importance_score": memory.importance_score,
                        "access_count": memory.access_count,
                        "last_accessed": memory.last_accessed,
                        "ai_generated": memory.ai_generated,
                        "user_confirmed": memory.user_confirmed,
                    }

                return web_ui_data

        except Exception as e:
            logger.error(f"Failed to get web UI memory data: {e}")
            return {}

    def _passes_web_ui_filters(
        self, web_data: Dict[str, Any], query: WebUIMemoryQuery
    ) -> bool:
        """Check if memory passes web UI specific filters."""
        # UI source filter
        if query.ui_source and web_data.get("ui_source") != query.ui_source.value:
            return False

        # Memory type filter
        if query.memory_types:
            memory_type = web_data.get("memory_type", "general")
            if memory_type not in [mt.value for mt in query.memory_types]:
                return False

        # Importance range filter
        if query.importance_range:
            importance = web_data.get("importance_score", 5)
            if not (
                query.importance_range[0] <= importance <= query.importance_range[1]
            ):
                return False

        # User confirmed filter
        if query.only_user_confirmed and not web_data.get("user_confirmed", True):
            return False

        # AI generated filter
        if query.only_ai_generated and not web_data.get("ai_generated", False):
            return False

        # Conversation filter
        if (
            query.conversation_id
            and web_data.get("conversation_id") != query.conversation_id
        ):
            return False

        return True

    async def _increment_access_count(
        self, tenant_id: Union[str, uuid.UUID], memory_id: str
    ):
        """Increment access count for a memory."""
        try:
            async with self.db_client.get_async_session() as session:
                await session.execute(
                    update(TenantMemoryEntry)
                    .where(TenantMemoryEntry.vector_id == memory_id)
                    .values(
                        access_count=TenantMemoryEntry.access_count + 1,
                        last_accessed=datetime.utcnow(),
                    )
                )
                await session.commit()

        except Exception as e:
            logger.error(f"Failed to increment access count: {e}")

    async def _generate_auto_tags(
        self, content: str, memory_type: MemoryType
    ) -> List[str]:
        """Generate automatic tags for memory content using spaCy NLP."""
        try:
            # Import spaCy service for entity-based tagging
            from ai_karen_engine.services.nlp_service_manager import nlp_service_manager

            tags = []

            # Extract entities and use them as tags
            entities = await nlp_service_manager.spacy_service.extract_entities(content)
            for entity_text, entity_label in entities:
                # Add entity labels as tags
                tags.append(entity_label.lower())
                # Add entity text as tag if it's short enough
                if len(entity_text) <= 20:
                    tags.append(entity_text.lower().replace(" ", "_"))

            # Get linguistic features for additional tagging
            features = await nlp_service_manager.spacy_service.get_linguistic_features(
                content
            )

            # Add POS-based tags
            pos_distribution = features.get("pos_distribution", {})
            if pos_distribution.get("VERB", 0) > pos_distribution.get("NOUN", 0):
                tags.append("action_oriented")
            elif pos_distribution.get("NOUN", 0) > 2:
                tags.append("fact_heavy")

            # Add sentiment-based tags (basic keyword matching)
            positive_keywords = [
                "like",
                "love",
                "enjoy",
                "prefer",
                "good",
                "great",
                "excellent",
            ]
            negative_keywords = ["dislike", "hate", "avoid", "bad", "terrible", "awful"]

            content_lower = content.lower()
            if any(keyword in content_lower for keyword in positive_keywords):
                tags.append("positive")
            if any(keyword in content_lower for keyword in negative_keywords):
                tags.append("negative")

            # Fallback to basic keyword extraction if spaCy fails
            if not tags:
                keywords = [
                    "important",
                    "remember",
                    "fact",
                    "preference",
                    "like",
                    "dislike",
                    "always",
                    "never",
                ]
                for keyword in keywords:
                    if keyword.lower() in content_lower:
                        tags.append(keyword)

            # Add memory type as tag
            tags.append(memory_type.value)

            # Add length-based tags
            if len(content) > 200:
                tags.append("detailed")
            elif len(content) < 50:
                tags.append("brief")

            # Add sentence count based tags
            sentence_count = features.get("sentence_count", 1)
            if sentence_count > 3:
                tags.append("multi_sentence")
            elif sentence_count == 1:
                tags.append("single_sentence")

            return list(set(tags))  # Remove duplicates

        except Exception as e:
            logger.warning(
                f"Failed to generate auto tags using spaCy, using fallback: {e}"
            )
            # Fallback to simple keyword extraction
            tags = []
            keywords = [
                "important",
                "remember",
                "fact",
                "preference",
                "like",
                "dislike",
                "always",
                "never",
            ]
            content_lower = content.lower()
            for keyword in keywords:
                if keyword in content_lower:
                    tags.append(keyword)

            tags.append(memory_type.value)

            if len(content) > 200:
                tags.append("detailed")
            elif len(content) < 50:
                tags.append("brief")

            return list(set(tags))

    async def _extract_facts(self, content: str) -> List[str]:
        """Extract facts from memory content using spaCy NLP."""
        try:
            # Import spaCy service for fact extraction
            from ai_karen_engine.services.nlp_service_manager import nlp_service_manager

            # Use spaCy service to extract facts
            facts = await nlp_service_manager.spacy_service.extract_facts(content)

            # Convert fact dictionaries to strings for storage
            fact_strings = []
            for fact in facts:
                if fact["type"] == "entity":
                    fact_strings.append(f"{fact['entity']} is a {fact['label']}")
                elif fact["type"] == "relationship":
                    fact_strings.append(
                        f"{fact['subject']} {fact['relation']} {fact['object']}"
                    )

            # Fallback to simple pattern matching if spaCy extraction fails
            if not fact_strings:
                fact_patterns = [
                    "I am",
                    "I like",
                    "I dislike",
                    "I prefer",
                    "My name is",
                    "I work at",
                    "I live in",
                    "I was born",
                    "I studied",
                ]

                for pattern in fact_patterns:
                    if pattern.lower() in content.lower():
                        # Extract sentence containing the pattern
                        sentences = content.split(".")
                        for sentence in sentences:
                            if pattern.lower() in sentence.lower():
                                fact_strings.append(sentence.strip())
                                break

            return fact_strings

        except Exception as e:
            logger.warning(f"Failed to extract facts using spaCy, using fallback: {e}")
            # Fallback to simple pattern matching
            facts = []
            fact_patterns = [
                "I am",
                "I like",
                "I dislike",
                "I prefer",
                "My name is",
                "I work at",
                "I live in",
                "I was born",
                "I studied",
            ]

            for pattern in fact_patterns:
                if pattern.lower() in content.lower():
                    sentences = content.split(".")
                    for sentence in sentences:
                        if pattern.lower() in sentence.lower():
                            facts.append(sentence.strip())
                            break

            return facts

    def get_metrics(self) -> Dict[str, Any]:
        """Get combined metrics from base manager and web UI service."""
        base_metrics = self.base_manager.metrics.copy()
        base_metrics.update(self.web_ui_metrics)
        return base_metrics


__all__ = [
    "WebUIMemoryService",
    "WebUIMemoryEntry",
    "WebUIMemoryQuery",
    "MemoryType",
    "UISource",
    "MemoryContextBuilder",
]
