"""Mock LLM provider used for offline development and automated testing.

The provider implements the minimal surface that :class:`LLMProviderBase`
expects so that higher level orchestrators can exercise the full request
pipeline even when real model backends are unavailable (for example during
CI runs or when proprietary API keys are missing).
"""

from __future__ import annotations

import itertools
import json
import time
from typing import Iterator, List, Optional

from ai_karen_engine.integrations.llm_utils import (
    LLMProviderBase,
    record_llm_metric,
)


class MockLLMProvider(LLMProviderBase):
    """Deterministic provider that synthesises friendly responses."""

    def __init__(self, model: str = "mock-response", latency_ms: int = 50) -> None:
        self.model = model
        self.latency_ms = latency_ms
        self._counter = itertools.count()
        self.last_usage: dict[str, int] = {}

    # ------------------------------------------------------------------
    # Generation helpers
    # ------------------------------------------------------------------
    def _build_response(self, prompt: str) -> str:
        prompt_preview = prompt.strip()
        if len(prompt_preview) > 160:
            prompt_preview = f"{prompt_preview[:157]}..."

        idx = next(self._counter) % 3
        templates = [
            "Thanks for your message! Here's a thoughtful response: {content}",
            "I understand your prompt and this is my reply: {content}",
            "Processed successfully. Response follows: {content}",
        ]
        content = (
            "This is a simulated answer generated by Kari's mock provider. "
            "It proves that the request pipeline is working end-to-end."
        )
        return templates[idx].format(content=content) + f"\n\n(Prompt excerpt: {prompt_preview})"

    def _record_usage(self, prompt: str, completion: str) -> None:
        prompt_tokens = max(1, len(prompt.split()))
        completion_tokens = max(1, len(completion.split()))
        self.last_usage = {
            "prompt_tokens": prompt_tokens,
            "completion_tokens": completion_tokens,
            "total_tokens": prompt_tokens + completion_tokens,
            "cost": 0,
        }

    # ------------------------------------------------------------------
    # Provider API
    # ------------------------------------------------------------------
    def generate_text(self, prompt: str, **kwargs) -> str:  # type: ignore[override]
        start = time.time()
        response = self._build_response(prompt)
        self._record_usage(prompt, response)
        time.sleep(self.latency_ms / 1000.0)
        record_llm_metric("generate_text", time.time() - start, True, "mock")
        return response

    def stream_generate(self, prompt: str, **kwargs) -> Iterator[str]:
        """Stream response tokens one by one."""
        text = self.generate_text(prompt, **kwargs)
        for token in text.split():
            yield token + " "

    def embed(self, text: str | List[str], **kwargs) -> List[float]:  # type: ignore[override]
        if isinstance(text, list):
            combined = " ".join(text)
        else:
            combined = text
        token_count = max(1, len(combined.split()))
        # Return deterministic pseudo embedding so downstream code can run.
        return [float((token_count + i) % 13) for i in range(12)]

    # Convenience hook used by diagnostics routes
    def get_provider_info(self) -> dict[str, Optional[str]]:
        return {
            "provider": "mock",
            "model": self.model,
            "description": "Deterministic responses for offline testing",
        }

    # Compatibility shim for registry health checks
    def ping(self) -> bool:
        return True

    def available_models(self) -> List[str]:
        return [self.model]

    # Compatibility with legacy serialisation helpers
    def dumps(self, payload: dict[str, str]) -> str:
        return json.dumps(payload)

    def loads(self, payload: str) -> dict[str, str]:
        return json.loads(payload)


__all__ = ["MockLLMProvider"]
