{
  "name": "LlamaCppLocal",
  "provider_id": "llama_cpp",
  "type": "llm_provider",
  "entrypoint": "llama.handler:router",
  "description": "Pure in-process LLM using llama-cpp-python. No REST, no server. Hot-reload, async, and streaming.",
  "runtimes": ["python3.10+", "llama-cpp-python>=0.2.60"],
  "tags": ["local", "llm", "plugin", "production"],
  "config": {
    "model_path": "./models/tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf",
    "n_ctx": 2048,
    "n_gpu_layers": 30,
    "max_tokens": 1024,
    "temperature": 0.7
  }
}
