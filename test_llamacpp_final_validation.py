#!/usr/bin/env python3
"""
Final comprehensive validation for LlamaCpp migration
Tests all aspects without requiring imports or running server
"""

import os
import sys
import json
import re
from pathlib import Path
from typing import Dict, Any, List, Tuple

class FinalMigrationValidator:
    def __init__(self):
        self.results = {
            "task_8_requirements": {},
            "overall_status": "PENDING"
        }
        
    def run_validation(self) -> Dict[str, Any]:
        """Run final validation for task 8 requirements"""
        print("ğŸ¯ Final LlamaCpp Migration Validation")
        print("Testing Task 8 Requirements:")
        print("- Test all API endpoints work with new /llm/llamacpp/* routes")
        print("- Verify backward compatibility aliases function correctly") 
        print("- Test configuration migration scenarios work as expected")
        print("- Verify all logging and metrics use new naming conventions")
        print("=" * 70)
        
        # Requirement 1.1, 1.3: API endpoints and logging/metrics naming
        api_logging_result = self._test_api_endpoints_and_logging()
        self.results["task_8_requirements"]["api_endpoints_logging"] = api_logging_result
        print(f"  {api_logging_result['status']}: API Endpoints & Logging - {api_logging_result.get('summary', '')}")
        
        # Requirement 2.2: Plugin system routes
        plugin_routes_result = self._test_plugin_routes()
        self.results["task_8_requirements"]["plugin_routes"] = plugin_routes_result
        print(f"  {plugin_routes_result['status']}: Plugin Routes - {plugin_routes_result.get('summary', '')}")
        
        # Requirement 5.3: Configuration migration
        config_result = self._test_configuration_migration()
        self.results["task_8_requirements"]["configuration_migration"] = config_result
        print(f"  {config_result['status']}: Configuration Migration - {config_result.get('summary', '')}")
        
        # Backward compatibility check
        backward_compat_result = self._test_backward_compatibility()
        self.results["task_8_requirements"]["backward_compatibility"] = backward_compat_result
        print(f"  {backward_compat_result['status']}: Backward Compatibility - {backward_compat_result.get('summary', '')}")
        
        # Overall assessment
        self._assess_final_status()
        
        return self.results
    
    def _test_api_endpoints_and_logging(self) -> Dict[str, Any]:
        """Test API endpoints work with new routes and logging uses new naming"""
        
        # Check both client files for proper updates
        client_files = [
            "src/marketplace/ai/llm-services/llama/llama_client.py",
            "src/ai_karen_engine/plugins/llm_services/llama/llama_client.py"
        ]
        
        plugin_file = "plugin_marketplace/ai/llm-services/llama/llama_plugin.py"
        
        checks = {
            "llamacpp_logger": False,
            "llamacpp_metrics": False,
            "llamacpp_routes": False,
            "no_ollama_refs": True,
            "log_messages_updated": False
        }
        
        # Check client files
        for client_file in client_files:
            if Path(client_file).exists():
                content = Path(client_file).read_text()
                
                # Check logger names
                if "llamacpp_inprocess" in content:
                    checks["llamacpp_logger"] = True
                
                # Check metrics
                if "llamacpp_requests_total" in content and "llamacpp_latency_seconds" in content:
                    checks["llamacpp_metrics"] = True
                
                # Check log messages
                if "[llamacpp_inprocess]" in content:
                    checks["log_messages_updated"] = True
                
                # Check for ollama references
                if "ollama" in content.lower():
                    checks["no_ollama_refs"] = False
        
        # Check plugin file
        if Path(plugin_file).exists():
            content = Path(plugin_file).read_text()
            
            # Check routes
            if '"/llm/llamacpp"' in content:
                checks["llamacpp_routes"] = True
            
            # Check for ollama references
            if "ollama" in content.lower():
                checks["no_ollama_refs"] = False
        
        passed_checks = sum(checks.values())
        total_checks = len(checks)
        
        if passed_checks == total_checks:
            return {"status": "PASS", "checks": checks, 
                   "summary": f"All {total_checks} API/logging checks passed"}
        else:
            failed = [k for k, v in checks.items() if not v]
            return {"status": "FAIL", "checks": checks, "failed": failed,
                   "summary": f"{passed_checks}/{total_checks} checks passed"}
    
    def _test_plugin_routes(self) -> Dict[str, Any]:
        """Test plugin routes are properly defined"""
        plugin_file = Path("plugin_marketplace/ai/llm-services/llama/llama_plugin.py")
        
        if not plugin_file.exists():
            return {"status": "SKIP", "reason": "Plugin file not found"}
        
        content = plugin_file.read_text()
        
        # Expected route endpoints
        expected_endpoints = ["health", "models", "chat", "achat", "embedding", "switch"]
        
        found_endpoints = []
        for endpoint in expected_endpoints:
            # Look for route decorators
            if f'@router.get("/{endpoint}"' in content or f'@router.post("/{endpoint}"' in content:
                found_endpoints.append(endpoint)
        
        # Check router prefix
        has_correct_prefix = 'prefix="/llm/llamacpp"' in content
        
        if len(found_endpoints) >= 5 and has_correct_prefix:
            return {"status": "PASS", "found_endpoints": found_endpoints,
                   "summary": f"Found {len(found_endpoints)} endpoints with correct prefix"}
        else:
            return {"status": "FAIL", "found_endpoints": found_endpoints, 
                   "has_prefix": has_correct_prefix,
                   "summary": f"Only {len(found_endpoints)} endpoints found, prefix: {has_correct_prefix}"}
    
    def _test_configuration_migration(self) -> Dict[str, Any]:
        """Test configuration migration scenarios"""
        
        # Check default provider setting
        schema_file = Path("scripts/maintenance/fix_auth_schema.py")
        config_checks = {
            "default_provider_llamacpp": False,
            "env_vars_llamacpp": False,
            "fallback_docs_updated": False
        }
        
        # Check schema file
        if schema_file.exists():
            content = schema_file.read_text()
            if 'preferredLLMProvider' in content and '"llamacpp"' in content:
                config_checks["default_provider_llamacpp"] = True
        
        # Check environment variables in client
        client_files = [
            "src/marketplace/ai/llm-services/llama/llama_client.py",
            "src/ai_karen_engine/plugins/llm_services/llama/llama_client.py"
        ]
        
        for client_file in client_files:
            if Path(client_file).exists():
                content = Path(client_file).read_text()
                llamacpp_vars = re.findall(r'LLAMACPP_\w+', content)
                if len(llamacpp_vars) >= 2:
                    config_checks["env_vars_llamacpp"] = True
                    break
        
        # Check fallback documentation
        fallback_doc = Path("docs/LLM_FALLBACK_HIERARCHY_IMPLEMENTATION.md")
        if fallback_doc.exists():
            content = fallback_doc.read_text()
            if "llamacpp" in content.lower():
                config_checks["fallback_docs_updated"] = True
        
        passed = sum(config_checks.values())
        total = len(config_checks)
        
        if passed == total:
            return {"status": "PASS", "checks": config_checks,
                   "summary": f"All {total} configuration checks passed"}
        else:
            failed = [k for k, v in config_checks.items() if not v]
            return {"status": "FAIL", "checks": config_checks, "failed": failed,
                   "summary": f"{passed}/{total} configuration checks passed"}
    
    def _test_backward_compatibility(self) -> Dict[str, Any]:
        """Test backward compatibility implementation"""
        plugin_file = Path("plugin_marketplace/ai/llm-services/llama/llama_plugin.py")
        
        if not plugin_file.exists():
            return {"status": "SKIP", "reason": "Plugin file not found"}
        
        content = plugin_file.read_text()
        
        # Look for backward compatibility features
        has_ollama_routes = "/llm/ollama" in content
        has_redirect_logic = any(word in content.lower() for word in ["redirect", "alias", "deprecated"])
        has_compatibility_comments = "backward" in content.lower() or "compatibility" in content.lower()
        
        if has_ollama_routes or has_redirect_logic or has_compatibility_comments:
            return {"status": "IMPLEMENTED", 
                   "features": {
                       "ollama_routes": has_ollama_routes,
                       "redirect_logic": has_redirect_logic,
                       "compatibility_comments": has_compatibility_comments
                   },
                   "summary": "Backward compatibility features found"}
        else:
            return {"status": "NOT_IMPLEMENTED", 
                   "summary": "No backward compatibility features found (tasks 6-7 pending)"}
    
    def _assess_final_status(self):
        """Assess final validation status"""
        results = self.results["task_8_requirements"]
        
        # Count statuses
        statuses = [result.get("status", "UNKNOWN") for result in results.values()]
        pass_count = statuses.count("PASS")
        fail_count = statuses.count("FAIL")
        skip_count = statuses.count("SKIP")
        other_count = len(statuses) - pass_count - fail_count - skip_count
        
        print(f"\nğŸ“Š Task 8 Validation Summary:")
        print(f"  Passed: {pass_count}")
        print(f"  Failed: {fail_count}")
        print(f"  Skipped: {skip_count}")
        print(f"  Other: {other_count}")
        
        # Determine overall status
        if fail_count == 0 and pass_count >= 3:  # At least 3 core requirements passed
            self.results["overall_status"] = "PASS"
            print("âœ… Task 8 Validation: PASS")
            print("   Core migration requirements validated successfully!")
            print("   Note: Backward compatibility (tasks 6-7) may still be pending")
        elif fail_count > 0:
            self.results["overall_status"] = "FAIL"
            print(f"âŒ Task 8 Validation: FAIL ({fail_count} failures)")
        else:
            self.results["overall_status"] = "PARTIAL"
            print("âš ï¸  Task 8 Validation: PARTIAL")

def main():
    """Main validation function"""
    validator = FinalMigrationValidator()
    results = validator.run_validation()
    
    # Save results
    results_file = Path("llamacpp_final_validation_results.json")
    with open(results_file, "w") as f:
        json.dump(results, f, indent=2)
    
    print(f"\nğŸ’¾ Results saved to: {results_file}")
    
    # Print recommendations
    if results["overall_status"] == "PASS":
        print(f"\nğŸ‰ Migration Validation Complete!")
        print(f"   The core LlamaCpp migration has been successfully implemented.")
        print(f"   Tasks 6 and 7 (backward compatibility) can be implemented next.")
    elif results["overall_status"] == "FAIL":
        print(f"\nğŸ”§ Action Required:")
        for req_name, req_result in results["task_8_requirements"].items():
            if req_result.get("status") == "FAIL":
                print(f"   - Fix {req_name}: {req_result.get('summary', 'No details')}")
    
    # Exit codes
    if results["overall_status"] == "PASS":
        sys.exit(0)
    elif results["overall_status"] == "FAIL":
        sys.exit(1)
    else:
        sys.exit(2)

if __name__ == "__main__":
    main()