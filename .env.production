# Kari AI - Production Environment Configuration
# Chat Runtime Production Wiring

# Environment
ENVIRONMENT=production
KARI_ENV=production

# API Configuration
HOST=0.0.0.0
PORT=8000
API_BASE_URL=http://localhost:8000

# Chat Runtime Features
ENABLE_STREAMING=true
ENABLE_FALLBACK=true
ENABLE_MEMORY=true

# Streaming Configuration
STREAM_TIMEOUT=30
MAX_CONCURRENT_STREAMS=100

# Memory Configuration
ENABLE_SHORT_TERM_MEMORY=true
ENABLE_LONG_TERM_MEMORY=true
ENABLE_VECTOR_MEMORY=true

# Memory Backends
REDIS_ENABLED=true
REDIS_HOST=localhost
REDIS_PORT=6379
REDIS_DB=0

MILVUS_ENABLED=true
MILVUS_HOST=localhost
MILVUS_PORT=19530

DUCKDB_ENABLED=true
DUCKDB_PATH=./data/memory.duckdb

# Observability
ENABLE_METRICS=true
ENABLE_PROMETHEUS=true
PROMETHEUS_PORT=9090
ENABLE_STRUCTURED_LOGGING=true
LOG_LEVEL=INFO
LOG_FORMAT=json

# Performance
MAX_MESSAGE_LENGTH=10000
MAX_TOKENS_DEFAULT=4096
RATE_LIMIT_REQUESTS=10
RATE_LIMIT_WINDOW=60

# Resource Management
MAX_WORKERS=4
MAX_CONCURRENT_REQUESTS=50

# Fallback Settings
FALLBACK_ENABLED=true
FALLBACK_TIMEOUT=5
DEGRADED_MODE_ENABLED=true

# Provider Configuration
DEFAULT_PROVIDER=local
DEFAULT_MODEL=tinyllama-1.1b

# LLM Providers (Priority Order)
ENABLE_LOCAL_MODELS=true
ENABLE_HUGGINGFACE=true
ENABLE_OPENAI=false
ENABLE_GEMINI=false

# Model Paths
MODELS_PATH=./models
LLAMA_CPP_MODELS_PATH=./models/llama-cpp
TRANSFORMERS_MODELS_PATH=./models/transformers

# Security
SECRET_KEY=your-secret-key-here
API_KEY_HEADER=X-API-KEY
AUTH_MODE=production

# CORS
CORS_ORIGINS=["http://localhost:3000","http://localhost:8000"]

# Database
DB_ENABLED=true
DB_CONNECTION_TIMEOUT=30
DB_HEALTH_CHECK_INTERVAL=60
DB_MAX_CONNECTION_FAILURES=3

# Circuit Breaker
CIRCUIT_BREAKER_ENABLED=true
CIRCUIT_BREAKER_THRESHOLD=5
CIRCUIT_BREAKER_TIMEOUT=60

# Graceful Degradation
GRACEFUL_DEGRADATION_ENABLED=true
DEGRADED_MODE_TIMEOUT=300
