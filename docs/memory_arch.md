# Memory Architecture


Kari uses a multi-tier memory system to balance speed and recall quality.

1. **NeuroVault** – an in-memory buffer used by `ChatHub` for short-term context. Records decay with `v(t)=v₀ e^{-0.05t}` (days) and can be purged via slash command.
2. **MilvusClient** – an in-process vector store with TTL pruning. It stores embeddings along with metadata such as `timestamp`, `tag` and optional `ttl_override`.
3. **Redis Cache** – a lightweight cache for hot items and recent events. This is currently a stub implementation in memory; the API mirrors Redis commands.
4. **SessionBuffer** – a DuckDB-backed queue that holds chat logs until they can be flushed to Postgres. Flushes occur on session end or when the buffer reaches a configurable size.
5. **Postgres** – relational store for structured logs and plugin state. Used when the engine needs ACID compliance or joins across metadata.
6. **Elasticsearch** – optional full-text index for transcripts and document archives. Provides rich keyword search alongside dense vector recall.

### Storage Responsibilities

- **Dense vectors** → **MilvusClient**
- **Relational data / metadata** → **Postgres**
- **Full-text search** → **Elasticsearch**
- **Hot cache / events** → **Redis Cache**
 

 
Embeddings are generated by `EmbeddingManager`, which deterministically hashes text into a fixed-size vector. When new text is ingested the engine calculates a _surprise_ score based on existing vectors. Low-surprise inputs are ignored to keep the store compact.

### Ingestion

```python
engine.ingest("remember this", {"tag": "demo"}, ttl_seconds=3600)
```

The record is stored with the current timestamp. A background call to `prune()` removes entries older than the TTL or custom override.

### Query

```python
results = engine.query("memory", top_k=5, metadata_filter={"tag": "demo"})
```

Results are ranked by cosine similarity and recency using an exponential decay controlled by `recency_alpha`.

### Decay & Surprise Weighting

- **TTL** determines when records expire.
- **Recency weight** favors newer memories during search.
- **Surprise score** prevents storing data too similar to existing vectors.

This system keeps the memory footprint small while allowing high recall accuracy. See [docs/observability.md](observability.md) for metrics exposed during upsert and search operations.

## ElasticSearch Integration

When ElasticSearch is available the memory manager uses it as the first lookup layer. Configure the connection via environment variables:

- `ELASTIC_HOST` – host of the Elastic cluster (default `localhost`)
- `ELASTIC_PORT` – port of the cluster (default `9200`)
- `ELASTIC_USER` – basic auth user (optional)
- `ELASTIC_PASSWORD` – password for the user (optional)
- `ELASTIC_INDEX` – index name for Kari memory (default `kari_memory`)

If these variables are unset the client falls back to its in-memory stub.
