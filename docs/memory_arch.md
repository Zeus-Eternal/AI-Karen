# Memory Architecture


Kari uses a multi-tier memory system to balance speed and recall quality.

1. **NeuroVault** – an in-memory buffer used by `ChatHub` for short-term context. Records decay with `v(t)=v₀ e^{-0.05t}` (days) and can be purged via slash command.
2. **MilvusClient** – an in-process vector store with TTL pruning. It stores embeddings along with metadata such as `timestamp`, `tag` and optional `ttl_override`.
3. **Redis Cache** – a lightweight cache for hot items and recent events. This is currently a stub implementation in memory; the API mirrors Redis commands.
 

 
Embeddings are generated by `EmbeddingManager`, which deterministically hashes text into a fixed-size vector. When new text is ingested the engine calculates a _surprise_ score based on existing vectors. Low-surprise inputs are ignored to keep the store compact.

### Ingestion

```python
engine.ingest("remember this", {"tag": "demo"}, ttl_seconds=3600)
```

The record is stored with the current timestamp. A background call to `prune()` removes entries older than the TTL or custom override.

### Query

```python
results = engine.query("memory", top_k=5, metadata_filter={"tag": "demo"})
```

Results are ranked by cosine similarity and recency using an exponential decay controlled by `recency_alpha`.

### Decay & Surprise Weighting

- **TTL** determines when records expire.
- **Recency weight** favors newer memories during search.
- **Surprise score** prevents storing data too similar to existing vectors.

This system keeps the memory footprint small while allowing high recall accuracy. See [docs/observability.md](observability.md) for metrics exposed during upsert and search operations.
